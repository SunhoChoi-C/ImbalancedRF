library(randomForest)
library(caret)

set.seed(123)
data <- twoClassSim(1000, intercept = -15, linearVars = 15, noiseVars = 5)
names(data)[names(data)=="Class"]="y"
data$y = as.factor(ifelse(data$y=="Class1",0,1))

BRF <- function(data,ntree=100,prob=0.8){
  set.seed(123)
  trainIndex <- createDataPartition(data$y, p = prob, list = FALSE)
  trainData <- data[trainIndex, ]
  testData <- data[-trainIndex, ]
  testFeatures <- testData[, -which(names(testData) == "y")]
  nmin <- sum(trainData$y==1)
  
  brf_model <- randomForest(
    y ~ ., 
    data = trainData,
    importance = TRUE,
    ntree=ntree,
    strata=trainData$y,
    sampsize=c(nmin,nmin)
  )
  
  predictions <- predict(brf_model, newdata = testFeatures)
  conf_matrix <- confusionMatrix(predictions, testData$y)
  
  result <- list("model"=brf_model,"matrix"=conf_matrix)
  
}


result <- BRF(data,ntree=100,prob=0.8)

WRF <- function(data,weights,cv=5,ntree=100,prob=0.8){
  set.seed(123)
  trainIndex <- caret::createDataPartition(data$y, p = prob, list = FALSE)
  trainData <- data[trainIndex, ]
  testData <- data[-trainIndex, ]
  testFeatures <- testData[, -which(names(testData) == "y")]
  nmin <- sum(trainData$y==1)
  
  folds <- createFolds(trainData$y, k = cv)
  bestwt <- 1
  best_bal_acc <- 0
  bal_acc_weight=rep(0,length(weights))
  count<-1
  weights<-as.vector(weights)
  
  for (wt in weights){
    bal_acc=rep(0,cv)
    for (fold in folds){
      train_fold <- trainData[-fold, ]
      test_fold <- trainData[fold, ]
      
      wrf_model <- randomForest::randomForest(
        y ~ .,
        data = train_fold,
        importance = TRUE,
        ntree=ntree,
        classwt=c(wt,1)
      )
      
      predictions <- predict(wrf_model, test_fold)
      bal_acc[count] <- (sum(predictions == 1 & test_fold$y==1)/sum(test_fold$y==1)+ sum(predictions == 0 & test_fold$y==0)/sum(test_fold$y==0))/2
    }
    mean_bal_acc <- mean(bal_acc)
    
    if(bestwt==1 || mean_bal_acc>best_bal_acc){
      bestwt <- wt
      best_bal_acc <-mean_bal_acc
    }
    count<-count+1
  }
  wrf_model <- randomForest::randomForest(
    y ~ .,
    data = trainData,
    importance = TRUE,
    ntree=ntree,
    classwt=c(bestwt,1)
  )
  
  predictions <- stats::predict(wrf_model, newdata = testFeatures)
  conf_matrix <- caret::confusionMatrix(predictions, testData$y)
  
  result <- list("model"=wrf_model,"matrix"=conf_matrix)
  
  
}

result<- WRF(data,c(10000,20000))

MixedRF <- function(data,weights,alphas,k=5,ntree=100,prob=0.8){
  set.seed(123)
  trainIndex <- caret::createDataPartition(data$y, p = prob, list = FALSE)
  trainData <- data[trainIndex, ]
  testData <- data[-trainIndex, ]
  testFeatures <- testData[, -which(names(testData) == "y")]
  
  folds <- caret::createFolds(trainData$y, k = k)
  bestwt <- 1
  best_bal_acc <- 0
  bestalpha<-1
  best_bal_alpha<-0
  bal_acc_weight=rep(0,length(weights))
  count <- 1
  weights<-as.vector(weights)
  count2<-1
  
  #split the CV into two parts since double for loop takes too much time
  for(wt in weights){
    bal_acc=rep(0,k)
    for (fold in folds){
      train_fold <- trainData[-fold, ]
      test_fold <- trainData[fold, ]
      
      wrf_model <- randomForest::randomForest(
        y ~ .,
        data = train_fold,
        importance = TRUE,
        ntree=ntree,
        classwt=c(wt,1)
      )
      
      predictions <- stats::predict(wrf_model, test_fold)
      bal_acc[count] <- (sum(predictions == 1 & test_fold$y==1)/sum(test_fold$y==1)+ sum(predictions == 0 & test_fold$y==0)/sum(test_fold$y==0))/2
    }
    mean_bal_acc <- mean(bal_acc)
    
    if(bestwt==1 || mean_bal_acc>best_bal_acc){
      bestwt <- wt
      best_bal_acc <-mean_bal_acc
    }
    count <- count+1
  }
  wrf_model <- randomForest::randomForest(
    y ~ .,
    data = trainData,
    importance = TRUE,
    ntree=ntree,
    classwt=c(bestwt,1)
  )
  
  for(alpha in alphas){
    bal_acc=rep(0,k)
    for (fold in folds){
      train_fold <- trainData[-fold, ]
      test_fold <- trainData[fold, ]
      nmin<-sum(train_fold$y==1)
      nalpha <- as.integer(alpha*nmin)
      
      mix_model <- randomForest::randomForest(
        y ~ .,
        data = train_fold,
        importance = TRUE,
        ntree=ntree,
        strata=train_fold$y,
        sampsize=c(nalpha,nmin),
        classwt=c(wt,1)
      )
      
      predictions <- stats::predict(mix_model, test_fold)
      bal_acc[count2] <- (sum(predictions == 1 & test_fold$y==1)/sum(test_fold$y==1)+ sum(predictions == 0 & test_fold$y==0)/sum(test_fold$y==0))/2
    }
    mean_bal_acc <- mean(bal_acc)
    
    if(bestalpha==1 || mean_bal_acc>best_bal_alpha){
      bestalpha <- alpha
      best_bal_alpha <-mean_bal_acc
    }
    count2 <- count2+1
  }
  
  nmin <- sum(trainData$y==1)
  
  mix_model <- randomForest::randomForest(
    y ~ .,
    data = trainData,
    importance = TRUE,
    ntree=ntree,
    strata=trainData$y,
    sampsize=c(as.integer(bestalpha*nmin),nmin),
    classwt=c(bestwt,1)
  )
  
  predictions <- stats::predict(mix_model, newdata = testFeatures)
  conf_matrix <- caret::confusionMatrix(predictions, testData$y)
  
  result <- list("model"=mix_model,"matrix"=conf_matrix)
}

result<- MixedRF(data,c(100,1000,10000,100000),alphas=c(2,3,4))


ThresholdRF <- function(data,ntree=3000,prob=0.8){
  set.seed(123)
  trainIndex <- caret::createDataPartition(data$y, p = prob, list = FALSE)
  trainData <- data[trainIndex, ]
  testData <- data[-trainIndex, ]
  testFeatures <- testData[, -which(names(testData) == "y")]
  
  rf <- randomForestSRC::rfsrc(y ~., trainData, rfq =  TRUE, ntree = ntree, perf.type = "g.mean", importance = TRUE)
  threshold_rf<-stats::predict(rf,newdata=testData)
  
  threshold=1-(sum(trainData$y==1)/length(trainData$y))
  predic <- ifelse(threshold_rf$predicted[,2] > threshold,0,1)
  conf_matrix=caret::confusionMatrix(as.factor(predic),as.factor(testData$y))
  
  result <- list("model"=rf,"matrix"=conf_matrix)
}

result <- ThresholdRF(data)


